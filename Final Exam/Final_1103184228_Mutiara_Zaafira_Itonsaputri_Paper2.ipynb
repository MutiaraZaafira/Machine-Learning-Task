{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_1103184228_Mutiara Zaafira Itonsaputri_Paper2.ipynb",
      "provenance": [],
      "mount_file_id": "1N7eFdCHE9zbabi_eJHbrJ7Fho2Gv30LA",
      "authorship_tag": "ABX9TyPbTMhq04CGaLmRgxo0bjTV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MutiaraZaafira/Machine-Learning-Task/blob/main/Final%20Exam/Final_1103184228_Mutiara_Zaafira_Itonsaputri_Paper2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Intall Twython**"
      ],
      "metadata": {
        "id": "raeIrWaN_To2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install twython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLJ6eU4Du0Lk",
        "outputId": "b75babd6-6ff9-4004-b622-9c654c59ae64"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: twython in /usr/local/lib/python3.7/dist-packages (3.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from twython) (1.3.0)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from twython) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.4.0->twython) (3.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utils**"
      ],
      "metadata": {
        "id": "G4n4mQKg_XHF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bN6kHM4kXIAW"
      },
      "outputs": [],
      "source": [
        "import sys \n",
        "import pickle \n",
        "import random \n",
        "\n",
        "def file_to_wordset(filename):\n",
        "    words = []\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f:\n",
        "            words.append(line.strip())\n",
        "    return set(words)\n",
        "\n",
        "\n",
        "def write_status(i, total): \n",
        "    sys.stdout.write('\\r')\n",
        "    sys.stdout.write('Processing %d/%d' % (i, total))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def save_results_to_csv(results, csv_file): \n",
        "    with open(csv_file, 'w') as csv:\n",
        "        csv.write('id,prediction\\n')\n",
        "        for tweet_id, pred in results:\n",
        "            csv.write(tweet_id)\n",
        "            csv.write(',')\n",
        "            csv.write(str(pred))\n",
        "            csv.write('\\n')\n",
        "\n",
        "\n",
        "def top_n_words(pkl_file_name, N, shift=0): \n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    words = {p[0]: i + shift for i, p in enumerate(most_common)}\n",
        "    return words\n",
        "\n",
        "\n",
        "def top_n_bigrams(pkl_file_name, N, shift=0): \n",
        "    with open(pkl_file_name, 'rb') as pkl_file:\n",
        "        freq_dist = pickle.load(pkl_file)\n",
        "    most_common = freq_dist.most_common(N)\n",
        "    bigrams = {p[0]: i for i, p in enumerate(most_common)}\n",
        "    return bigrams\n",
        "\n",
        "\n",
        "def split_data(tweets, validation_split=0.1): \n",
        "    index = int((1 - validation_split) * len(tweets))\n",
        "    random.shuffle(tweets)\n",
        "    return tweets[:index], tweets[index:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "wShN6pJ6_Z-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUe-PryK_bb6",
        "outputId": "254515a8-cc32-4d2c-8dd9-31552cf13e6e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  File \"/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train.csv\", line 1\n",
            "    1,0,                     is so sad for my APL friend.............;;;;;;;;;;;;\n",
            "                              ^\n",
            "SyntaxError: invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/test.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAPRZ99z_wBB",
        "outputId": "bd0e0713-eb01-41da-f2be-9d46c8c6981f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  File \"/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/test.csv\", line 1\n",
            "    1,                     is so sad for my APL friend.............,,,,,,,,,,,,\n",
            "                            ^\n",
            "SyntaxError: invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VlU4JaWA-L6",
        "outputId": "90d7a260-84f6-41e8-cd4c-d3b2f841579b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stats**"
      ],
      "metadata": {
        "id": "8qmdx1zD_zoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1MQX2hg_09z",
        "outputId": "749fbce9-9ec3-4757-d35f-d9e7594065eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  File \"/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed.csv\", line 1\n",
            "    1,0,is so sad for my apl friend\n",
            "         ^\n",
            "SyntaxError: invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Baseline**"
      ],
      "metadata": {
        "id": "ffj4LY7gCWFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/test-processed.csv'\n",
        "POSITIVE_WORDS_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/positive-words.txt'\n",
        "NEGATIVE_WORDS_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/negative-words.txt'\n",
        "TRAIN = True\n",
        "\n",
        "\n",
        "def classify(processed_csv, test_file=True, **params): \n",
        "    positive_words = file_to_wordset(params.pop('positive_words'))\n",
        "    negative_words = file_to_wordset(params.pop('negative_words'))\n",
        "    predictions = []\n",
        "    with open(processed_csv, 'r') as csv: \n",
        "        for line in csv:\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.strip().split(',')\n",
        "            else:\n",
        "                tweet_id, label, tweet = line.strip().split(',')\n",
        "            pos_count, neg_count = 0, 0\n",
        "            for word in tweet.split():\n",
        "                if word in positive_words:\n",
        "                    pos_count += 1\n",
        "                elif word in negative_words:\n",
        "                    neg_count += 1\n",
        "            #print (pos_count, neg_count)\n",
        "            prediction = 1 if pos_count >= neg_count else 0\n",
        "            if test_file:\n",
        "                predictions.append((tweet_id, prediction))\n",
        "            else:\n",
        "                predictions.append((tweet_id, int(label), prediction))\n",
        "    return predictions\n",
        "\n",
        "\n",
        "if __name__ == '__main__': \n",
        "    if TRAIN:\n",
        "        predictions = classify(TRAIN_PROCESSED_FILE, test_file=(not TRAIN), positive_words=POSITIVE_WORDS_FILE, negative_words=NEGATIVE_WORDS_FILE)\n",
        "        correct = sum([1 for p in predictions if p[1] == p[2]]) * 100.0 / len(predictions)\n",
        "        print ('Correct = %.2f%%' % correct)\n",
        "    else:\n",
        "        predictions = classify(TEST_PROCESSED_FILE, test_file=(not TRAIN), positive_words=POSITIVE_WORDS_FILE, negative_words=NEGATIVE_WORDS_FILE)\n",
        "        save_results_to_csv(predictions, '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/baseline.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68tndNPG_8yO",
        "outputId": "bf5a7ada-dcf9-4471-9a86-097ba1d78795"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct = 65.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes**"
      ],
      "metadata": {
        "id": "mteGzA9rDY2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random \n",
        "import numpy as np \n",
        "from sklearn.naive_bayes import MultinomialNB \n",
        "from scipy.sparse import lil_matrix \n",
        "from sklearn.feature_extraction.text import TfidfTransformer \n",
        "import pandas as pd \n",
        "\n",
        "\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/test-processed.csv'\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "xrange = range\n",
        "\n",
        "def get_feature_vector(tweet): \n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'): \n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True): \n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = MultinomialNB()\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.partial_fit(training_set_X, training_set_y, classes=[0, 1])\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency': \n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/datasetnaivebayes.csv')\n",
        "        print ('\\nSaved to naivebayes.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BZYwuMQB-n9",
        "outputId": "0d04a6be-8772-4816-8566-a92b65502058"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7697/10000 = 76.9700 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/naivebayes.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvASNB0DDZ_A",
        "outputId": "5f9eee20-4609-4649-ce2e-81f4b1a89e5f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/naivebayes.py\", line 108, in <module>\n",
            "    unigrams = utils.top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
            "  File \"/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/utils.py\", line 44, in top_n_words\n",
            "    with open(pkl_file_name, 'rb') as pkl_file:\n",
            "IOError: [Errno 2] No such file or directory: '../train-processed-freqdist.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Maximum Entropy**"
      ],
      "metadata": {
        "id": "Dsq_yf-8FM-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk \n",
        "import random  \n",
        "import pickle\n",
        "import sys \n",
        "import numpy as np \n",
        "\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/test-processed.csv'\n",
        "USE_BIGRAMS = False\n",
        "TRAIN = True\n",
        "xrange = range\n",
        "\n",
        "def get_data_from_file(file_name, isTrain=True): \n",
        "    data = []\n",
        "    with open(train_csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if isTrain:\n",
        "                tag = line.split(',')[1]\n",
        "                bag_of_words = line.split(',')[2].split()\n",
        "                if USE_BIGRAMS:\n",
        "                    bag_of_words_bigram = list(nltk.bigrams(line.split(',')[2].split()))\n",
        "                    bag_of_words = bag_of_words+bag_of_words_bigram\n",
        "            else :\n",
        "                tag = '5'\n",
        "                bag_of_words = line.split(',')[1].split()\n",
        "                if USE_BIGRAMS:\n",
        "                    bag_of_words_bigram = list(nltk.bigrams(line.split(',')[1].split()))\n",
        "                    bag_of_words = bag_of_words+bag_of_words_bigram\n",
        "            data.append((bag_of_words, tag))\n",
        "    return data\n",
        "\n",
        "def split_data(tweets, validation_split=0.1): \n",
        "    index = int((1 - validation_split) * len(tweets))\n",
        "    random.shuffle(tweets)\n",
        "    return tweets[:index], tweets[index:]\n",
        "\n",
        "def list_to_dict(words_list): \n",
        "    return dict([(word, True) for word in words_list])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train = True\n",
        "    np.random.seed(1337)\n",
        "    train_csv_file = TRAIN_PROCESSED_FILE\n",
        "    test_csv_file = TEST_PROCESSED_FILE\n",
        "    train_data = get_data_from_file(train_csv_file, isTrain=True)\n",
        "    train_set, validation_set = split_data(train_data)\n",
        "    training_set_formatted = [(list_to_dict(element[0]), element[1]) for element in train_set]\n",
        "    validation_set_formatted = [(list_to_dict(element[0]), element[1]) for element in validation_set]\n",
        "    numIterations = 1\n",
        "    algorithm = nltk.classify.MaxentClassifier.ALGORITHMS[1]\n",
        "    classifier = nltk.MaxentClassifier.train(training_set_formatted, algorithm, max_iter=numIterations)\n",
        "    classifier.show_most_informative_features(10)\n",
        "    count = int(0)\n",
        "\n",
        "    for review in validation_set_formatted:\n",
        "        label = review[1]\n",
        "        text = review[0]\n",
        "        determined_label = classifier.classify(text)\n",
        "        #print(determined_label, label)\n",
        "        if determined_label!=label:\n",
        "            count+=int(1)\n",
        "    accuracy = (len(validation_set)-count)/len(validation_set)\n",
        "    print ('Validation set accuracy:%.4f'% (accuracy))\n",
        "    f = open('maxEnt_classifier.pickle', 'wb')\n",
        "    pickle.dump(classifier, f)\n",
        "    f.close()\n",
        "    print ('\\nPredicting for test data')\n",
        "    test_data = get_data_from_file(test_csv_file, isTrain=False)\n",
        "    test_set_formatted = [(list_to_dict(element[0]), element[1]) for element in test_data]\n",
        "    tweet_id = int(0)\n",
        "    results = []\n",
        "    for review in test_set_formatted:\n",
        "        text = review[0]\n",
        "        label = classifier.classify(text)\n",
        "        results.append((str(tweet_id), label))\n",
        "        tweet_id += int(1)\n",
        "    save_results_to_csv(results, '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/maxent.csv')\n",
        "    print ('\\nSaved to maxent.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAI99p1kFOrx",
        "outputId": "8500214a-cd04-4f84-d260-196b8c71137e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ==> Training (1 iterations)\n",
            "\n",
            "      Iteration    Log Likelihood    Accuracy\n",
            "      ---------------------------------------\n",
            "             1          -0.69315        0.564\n",
            "         Final          -0.61615        0.759\n",
            "  -1.350 shaundiviney==True and label is '0'\n",
            "   1.000 ephesians==True and label is '1'\n",
            "   1.000 tired.its==True and label is '1'\n",
            "   1.000 john_holmberg_is_sexy==True and label is '1'\n",
            "   1.000 halfbloodprince==True and label is '1'\n",
            "   1.000 princecharming==True and label is '0'\n",
            "   1.000 atthepool==True and label is '1'\n",
            "   1.000 imisscath==True and label is '0'\n",
            "   1.000 thaipbs==True and label is '1'\n",
            "   1.000 harpersglobe==True and label is '1'\n",
            "Validation set accuracy:0.7091\n",
            "\n",
            "Predicting for test data\n",
            "\n",
            "Saved to maxent.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Tree**"
      ],
      "metadata": {
        "id": "0FtTP4HLG4iW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random \n",
        "import numpy as np \n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from scipy.sparse import lil_matrix \n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/test-processed.csv'\n",
        "\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "\n",
        "USE_BIGRAMS = False\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "xrange = range\n",
        "\n",
        "def get_feature_vector(tweet): \n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'): #melakukan ekstrasi feature tweet\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    \"\"\"\n",
        "    Fits X for TF-IDF vectorization and returns the transformer.\n",
        "    \"\"\"\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams =top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = DecisionTreeClassifier(max_depth=25)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/decisiontree.csv')\n",
        "        print ('\\nSaved to decisiontree.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utOwGLTsG5wv",
        "outputId": "14d5240d-a6e7-4af3-89d1-ef16dd267802"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 6862/10000 = 68.6200 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/decisiontree.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhePK6mhQtfR",
        "outputId": "aa478f8e-2496-436c-b00a-2ff005b2239f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/decisiontree.py\", line 115, in <module>\n",
            "    unigrams = utils.top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
            "  File \"/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/utils.py\", line 44, in top_n_words\n",
            "    with open(pkl_file_name, 'rb') as pkl_file:\n",
            "IOError: [Errno 2] No such file or directory: '../train-processed-freqdist.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest**"
      ],
      "metadata": {
        "id": "LJqlB_muQ0fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "from scipy.sparse import lil_matrix \n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/test-processed.csv'\n",
        "\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = False\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'presence'\n",
        "xrange = range\n",
        "\n",
        "def get_feature_vector(tweet): #untuk mendapatkan vektor pada tweet\n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'): #melakukan ekstrasi feature tweet\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = RandomForestClassifier(n_jobs=2, random_state=0)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/randomforest.csv')\n",
        "        print ('\\nSaved to randomforest.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjNBTv8LQ2Eo",
        "outputId": "7b44787b-f836-4459-8e21-ab4025c19927"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7541/10000 = 75.4100 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/randomforest.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsjZdrBXRwa7",
        "outputId": "496cf741-067b-4cf2-c4df-4dbb4c7c74a0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/randomforest.py\", line 108, in <module>\n",
            "    unigrams = utils.top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
            "  File \"/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/utils.py\", line 44, in top_n_words\n",
            "    with open(pkl_file_name, 'rb') as pkl_file:\n",
            "IOError: [Errno 2] No such file or directory: '../train-processed-freqdist.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBoots**"
      ],
      "metadata": {
        "id": "0QUelTzmUik9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random \n",
        "import numpy as np \n",
        "from xgboost import XGBClassifier\n",
        "from scipy.sparse import lil_matrix \n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/test-processed.csv'\n",
        "\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "xrange = range\n",
        "\n",
        "def get_feature_vector(tweet): \n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'):\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets =split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = XGBClassifier(max_depth=25, silent=False, n_estimators=400)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/xgboost.csv')\n",
        "        print ('\\nSaved to xgboost.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tb7azvmUkV1",
        "outputId": "4b357a03-7ed2-4a66-ae3e-38f917fb3b86"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7713/10000 = 77.1300 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVM**"
      ],
      "metadata": {
        "id": "IqcBck8jWQ1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm \n",
        "import random \n",
        "import numpy as np \n",
        "from scipy.sparse import lil_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer \n",
        "\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/test-processed.csv'\n",
        "\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = True\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "xrange = range\n",
        "\n",
        "def get_feature_vector(tweet): \n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'): #melakukan ekstrasi feature tweet\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = lil_matrix((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def apply_tf_idf(X):\n",
        "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
        "    transformer.fit(X)\n",
        "    return transformer\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    \"\"\"Returns a list of tuples of type (tweet_id, feature_vector)\n",
        "            or (tweet_id, sentiment, feature_vector)\n",
        "    Args:\n",
        "        csv_file (str): Name of processed csv file generated by preprocess.py\n",
        "        test_file (bool, optional): If processing test file\n",
        "    Returns:\n",
        "        list: Of tuples\n",
        "    \"\"\"\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    clf = svm.LinearSVC(C=0.1)\n",
        "    batch_size = len(train_tweets)\n",
        "    i = 1\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    for training_set_X, training_set_y in extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "        write_status(i, n_train_batches)\n",
        "        i += 1\n",
        "        if FEAT_TYPE == 'frequency':\n",
        "            tfidf = apply_tf_idf(training_set_X)\n",
        "            training_set_X = tfidf.transform(training_set_X)\n",
        "        clf.fit(training_set_X, training_set_y)\n",
        "    print ('\\n')\n",
        "    print ('Testing')\n",
        "    if TRAIN:\n",
        "        correct, total = 0, len(val_tweets)\n",
        "        i = 1\n",
        "        batch_size = len(val_tweets)\n",
        "        n_val_batches = int(np.ceil(len(val_tweets) / float(batch_size)))\n",
        "        for val_set_X, val_set_y in extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE, batch_size=batch_size):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                val_set_X = tfidf.transform(val_set_X)\n",
        "            prediction = clf.predict(val_set_X)\n",
        "            correct += np.sum(prediction == val_set_y)\n",
        "            write_status(i, n_val_batches)\n",
        "            i += 1\n",
        "        print ('\\nCorrect: %d/%d = %.4f %%' % (correct, total, correct * 100. / total))\n",
        "    else:\n",
        "        del train_tweets\n",
        "        test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "        n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "        predictions = np.array([])\n",
        "        print ('Predicting batches')\n",
        "        i = 1\n",
        "        for test_set_X, _ in extract_features(test_tweets, test_file=True, feat_type=FEAT_TYPE):\n",
        "            if FEAT_TYPE == 'frequency':\n",
        "                test_set_X = tfidf.transform(test_set_X)\n",
        "            prediction = clf.predict(test_set_X)\n",
        "            predictions = np.concatenate((predictions, prediction))\n",
        "            write_status(i, n_test_batches)\n",
        "            i += 1\n",
        "        predictions = [(str(j), int(predictions[j]))\n",
        "                       for j in range(len(test_tweets))]\n",
        "        save_results_to_csv(predictions, '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/svm.csv')\n",
        "        print ('\\nSaved to svm.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNSJBEV-WR6A",
        "outputId": "70918df6-83c2-4b31-e0cc-51d7dbc6c683"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Processing 1/1\n",
            "\n",
            "Testing\n",
            "Processing 1/1\n",
            "Correct: 7868/10000 = 78.6800 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python2 /content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/svm.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd1BlhXabPAw",
        "outputId": "a1ef6ecc-6ef6-4a6e-af69-faa42639369c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/svm.py\", line 108, in <module>\n",
            "    unigrams = utils.top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
            "  File \"/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/utils.py\", line 44, in top_n_words\n",
            "    with open(pkl_file_name, 'rb') as pkl_file:\n",
            "IOError: [Errno 2] No such file or directory: '../train-processed-freqdist.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi Layer Preceptorn**"
      ],
      "metadata": {
        "id": "IvYrfnqgbi2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "import sys \n",
        "import random \n",
        "import numpy as np \n",
        "\n",
        "# Performs classification using an MLP/1-hidden-layer NN.\n",
        "\n",
        "FREQ_DIST_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed-freqdist.pkl'\n",
        "BI_FREQ_DIST_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed-freqdist-bi.pkl'\n",
        "TRAIN_PROCESSED_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/train-processed.csv'\n",
        "TEST_PROCESSED_FILE = '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/test-processed.csv'\n",
        "\n",
        "TRAIN = True\n",
        "UNIGRAM_SIZE = 15000\n",
        "VOCAB_SIZE = UNIGRAM_SIZE\n",
        "USE_BIGRAMS = False\n",
        "if USE_BIGRAMS:\n",
        "    BIGRAM_SIZE = 10000\n",
        "    VOCAB_SIZE = UNIGRAM_SIZE + BIGRAM_SIZE\n",
        "FEAT_TYPE = 'frequency'\n",
        "xrange = range\n",
        "\n",
        "def get_feature_vector(tweet): \n",
        "    uni_feature_vector = []\n",
        "    bi_feature_vector = []\n",
        "    words = tweet.split()\n",
        "    for i in xrange(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if unigrams.get(word):\n",
        "            uni_feature_vector.append(word)\n",
        "        if USE_BIGRAMS:\n",
        "            if bigrams.get((word, next_word)):\n",
        "                bi_feature_vector.append((word, next_word))\n",
        "    if len(words) >= 1:\n",
        "        if unigrams.get(words[-1]):\n",
        "            uni_feature_vector.append(words[-1])\n",
        "    return uni_feature_vector, bi_feature_vector\n",
        "\n",
        "\n",
        "def extract_features(tweets, batch_size=500, test_file=True, feat_type='presence'): #melakukan ekstrasi feature tweet\n",
        "    num_batches = int(np.ceil(len(tweets) / float(batch_size)))\n",
        "    for i in xrange(num_batches):\n",
        "        batch = tweets[i * batch_size: (i + 1) * batch_size]\n",
        "        features = np.zeros((batch_size, VOCAB_SIZE))\n",
        "        labels = np.zeros(batch_size)\n",
        "        for j, tweet in enumerate(batch):\n",
        "            if test_file:\n",
        "                tweet_words = tweet[1][0]\n",
        "                tweet_bigrams = tweet[1][1]\n",
        "            else:\n",
        "                tweet_words = tweet[2][0]\n",
        "                tweet_bigrams = tweet[2][1]\n",
        "                labels[j] = tweet[1]\n",
        "            if feat_type == 'presence':\n",
        "                tweet_words = set(tweet_words)\n",
        "                tweet_bigrams = set(tweet_bigrams)\n",
        "            for word in tweet_words:\n",
        "                idx = unigrams.get(word)\n",
        "                if idx:\n",
        "                    features[j, idx] += 1\n",
        "            if USE_BIGRAMS:\n",
        "                for bigram in tweet_bigrams:\n",
        "                    idx = bigrams.get(bigram)\n",
        "                    if idx:\n",
        "                        features[j, UNIGRAM_SIZE + idx] += 1\n",
        "        yield features, labels\n",
        "\n",
        "\n",
        "def process_tweets(csv_file, test_file=True):\n",
        "    tweets = []\n",
        "    print ('Generating feature vectors')\n",
        "    with open(csv_file, 'r') as csv:\n",
        "        lines = csv.readlines()\n",
        "        total = len(lines)\n",
        "        for i, line in enumerate(lines):\n",
        "            if test_file:\n",
        "                tweet_id, tweet = line.split(',')\n",
        "            else:\n",
        "                tweet_id, sentiment, tweet = line.split(',')\n",
        "            feature_vector = get_feature_vector(tweet)\n",
        "            if test_file:\n",
        "                tweets.append((tweet_id, feature_vector))\n",
        "            else:\n",
        "                tweets.append((tweet_id, int(sentiment), feature_vector))\n",
        "            write_status(i + 1, total)\n",
        "    print ('\\n')\n",
        "    return tweets\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(500, input_dim=VOCAB_SIZE, activation='sigmoid'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_model(model, val_tweets):\n",
        "    correct, total = 0, len(val_tweets)\n",
        "    for val_set_X, val_set_y in extract_features(val_tweets, feat_type=FEAT_TYPE, test_file=False):\n",
        "        prediction = model.predict_on_batch(val_set_X)\n",
        "        prediction = np.round(prediction)\n",
        "        correct += np.sum(prediction == val_set_y[:, None])\n",
        "    return float(correct) / total\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1337)\n",
        "    unigrams = top_n_words(FREQ_DIST_FILE, UNIGRAM_SIZE)\n",
        "    if USE_BIGRAMS:\n",
        "        bigrams = top_n_bigrams(BI_FREQ_DIST_FILE, BIGRAM_SIZE)\n",
        "    tweets = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
        "    if TRAIN:\n",
        "        train_tweets, val_tweets = split_data(tweets)\n",
        "    else:\n",
        "        random.shuffle(tweets)\n",
        "        train_tweets = tweets\n",
        "    del tweets\n",
        "    print ('Extracting features & training batches')\n",
        "    nb_epochs = 5\n",
        "    batch_size = 500\n",
        "    model = build_model()\n",
        "    n_train_batches = int(np.ceil(len(train_tweets) / float(batch_size)))\n",
        "    best_val_acc = 0.0\n",
        "    for j in xrange(nb_epochs):\n",
        "        i = 1\n",
        "        for training_set_X, training_set_y in extract_features(train_tweets, feat_type=FEAT_TYPE, batch_size=batch_size, test_file=False):\n",
        "            o = model.train_on_batch(training_set_X, training_set_y)\n",
        "            sys.stdout.write('\\rIteration %d/%d, loss:%.4f, acc:%.4f' %\n",
        "                             (i, n_train_batches, o[0], o[1]))\n",
        "            sys.stdout.flush()\n",
        "            i += 1\n",
        "        val_acc = evaluate_model(model, val_tweets)\n",
        "        print ('\\nEpoch: %d, val_acc:%.4f' % (j + 1, val_acc))\n",
        "        random.shuffle(train_tweets)\n",
        "        if val_acc > best_val_acc:\n",
        "            print ('Accuracy improved from %.4f to %.4f, saving model' % (best_val_acc, val_acc))\n",
        "            best_val_acc = val_acc\n",
        "            model.save('best_model.h5')\n",
        "    print ('Testing')\n",
        "    del train_tweets\n",
        "    del model\n",
        "    model = load_model('best_model.h5')\n",
        "    test_tweets = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
        "    n_test_batches = int(np.ceil(len(test_tweets) / float(batch_size)))\n",
        "    predictions = np.array([])\n",
        "    print ('Predicting batches')\n",
        "    i = 1\n",
        "    for test_set_X, _ in extract_features(test_tweets, feat_type=FEAT_TYPE, batch_size=batch_size, test_file=True):\n",
        "        prediction = np.round(model.predict_on_batch(test_set_X).flatten())\n",
        "        predictions = np.concatenate((predictions, prediction))\n",
        "        write_status(i, n_test_batches)\n",
        "        i += 1\n",
        "    predictions = [(str(j), int(predictions[j]))\n",
        "                   for j in range(len(test_tweets))]\n",
        "    save_results_to_csv(predictions, '/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/dataset/layerpre1.csv')\n",
        "    print ('\\nSaved to layerpre1.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRv_-EAebkjL",
        "outputId": "8a5f5d8c-deae-4528-d401-061210b62a1c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating feature vectors\n",
            "Processing 100000/100000\n",
            "\n",
            "Extracting features & training batches\n",
            "Iteration 180/180, loss:0.5283, acc:0.7280\n",
            "Epoch: 1, val_acc:0.7534\n",
            "Accuracy improved from 0.0000 to 0.7534, saving model\n",
            "Iteration 180/180, loss:0.4603, acc:0.7980\n",
            "Epoch: 2, val_acc:0.7644\n",
            "Accuracy improved from 0.7534 to 0.7644, saving model\n",
            "Iteration 180/180, loss:0.4389, acc:0.8020\n",
            "Epoch: 3, val_acc:0.7636\n",
            "Iteration 180/180, loss:0.4699, acc:0.7860\n",
            "Epoch: 4, val_acc:0.7610\n",
            "Iteration 180/180, loss:0.4741, acc:0.7720\n",
            "Epoch: 5, val_acc:0.7609\n",
            "Testing\n",
            "Generating feature vectors\n",
            "Processing 300000/300000\n",
            "\n",
            "Predicting batches\n",
            "Processing 600/600\n",
            "Saved to layerpre1.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/neuralnet.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHKlm1QseGgD",
        "outputId": "2f44a7b9-0ec4-4391-9ead-505c4e3137c4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  File \"/content/drive/MyDrive/Twitter-Sentiment-Analysis-main/Twitter-Sentiment-Analysis-main/code/neuralnet.py\", line 73\n",
            "    print 'Generating feature vectors'\n",
            "                                     ^\n",
            "SyntaxError: Missing parentheses in call to 'print'. Did you mean print('Generating feature vectors')?\n"
          ]
        }
      ]
    }
  ]
}